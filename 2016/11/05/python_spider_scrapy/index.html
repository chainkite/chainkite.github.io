<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>None.get</title>
  <meta name="author" content="chainkite">
  
  <meta name="description" content="Above all, I am a beginner in python. scrapy is a crawler framework in python, makes me write a crawler project quite easily in python. But, the progr">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="None.get"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="None.get" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">None.get</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-11-05T14:22:00.000Z"><a href="/2016/11/05/python_spider_scrapy/">2016-11-05</a></time>
      
      
  
    <h1 class="title"></h1>
  

    </header>
    <div class="entry">
      
        <p>Above all, I am a beginner in <code>python</code>. <code>scrapy</code> is a crawler framework in <code>python</code>, makes me write a crawler project quite easily in <code>python</code>. But, the program is not quite efficient at the begining. It’s built on <code>twisted</code>, which is async, but single-threaded…</p>
<p><strong>Start</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br><span class="line"></span><br><span class="line">scrapy startproject first_spider ~/works/first_spider</span><br></pre></td></tr></table></figure></p>
<p>Then you will get a project structure like this.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── first_spider</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── settings.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">└── scrapy.cfg</span><br></pre></td></tr></table></figure></p>
<p>Follow the structure, you will find it’s easy to implement your spider, because the only thing you need to consider is the logic you are going to crawl and save.</p>
<p><strong>There are a lot of tutorials teach you how to write a spider, so I will skip this and jump to something further</strong></p>
<p><strong>Speed up your spider</strong><br>What I found out when I run my spider with <code>scrapy crawl first_spider</code>, it is run in single-thread. Async is only working to prevent the parsing from downloading (prevent cpu stucking from io).<br>In order speed up my spider, I split my spider into several tiny spiders. (example below skip details)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── first_spider</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       ├── first_spiders.py</span><br><span class="line">│       └── support</span><br><span class="line">│           ├── __init__.py</span><br><span class="line">│           └── first_spider_support.py</span><br></pre></td></tr></table></figure></p>
<p>first_spider_support.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class FirstSpiderSupport:</span><br><span class="line">	def __init__(self):</span><br><span class="line">		pass</span><br><span class="line">	</span><br><span class="line">	def do_start_requests(self):</span><br><span class="line">		...</span><br><span class="line">	</span><br><span class="line">	def do_parse(self, response):</span><br><span class="line">		...</span><br></pre></td></tr></table></figure></p>
<p>first_spiders.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class FirstOneSpider(scrapy.Spider, FirstSpiderSupport):</span><br><span class="line">	name = &quot;first_one_spider&quot;</span><br><span class="line"></span><br><span class="line">	def start_requests(self):</span><br><span class="line">        return self.do_start_requests()</span><br><span class="line"></span><br><span class="line">	def parse(self, response):</span><br><span class="line">        return self.do_parse(response)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class FirstTwoSpider(scrapy.Spider, FirstSpiderSupport):</span><br><span class="line">	name = &quot;first_two_spider&quot;</span><br><span class="line"></span><br><span class="line">	def start_requests(self):</span><br><span class="line">        return self.do_start_requests()</span><br><span class="line"></span><br><span class="line">	def parse(self, response):</span><br><span class="line">        return self.do_parse(response)</span><br><span class="line">        </span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>Implement a <code>scrapy command</code> to run all spiders once.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── first_spider</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── commands</span><br><span class="line">│   │   ├── __init__.py</span><br><span class="line">│   │   └── crawlall.py</span><br></pre></td></tr></table></figure></p>
<p>crawlall.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from scrapy.commands import ScrapyCommand</span><br><span class="line">from scrapy.utils.conf import arglist_to_dict</span><br><span class="line">from scrapy.utils.python import without_none_values</span><br><span class="line">from scrapy.exceptions import UsageError</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line">class Command(ScrapyCommand):</span><br><span class="line"></span><br><span class="line">    requires_project = True</span><br><span class="line"></span><br><span class="line">    def syntax(self):</span><br><span class="line">        return &apos;[options]&apos;</span><br><span class="line"></span><br><span class="line">    def short_desc(self):</span><br><span class="line">        return &apos;Runs all of the spiders&apos;</span><br><span class="line"></span><br><span class="line">    def add_options(self, parser):</span><br><span class="line">        ScrapyCommand.add_options(self, parser)</span><br><span class="line">        parser.add_option(&quot;-a&quot;, dest=&quot;spargs&quot;, action=&quot;append&quot;, default=[], metavar=&quot;NAME=VALUE&quot;,</span><br><span class="line">                          help=&quot;set spider argument (may be repeated)&quot;)</span><br><span class="line">        parser.add_option(&quot;-o&quot;, &quot;--output&quot;, metavar=&quot;FILE&quot;,</span><br><span class="line">                          help=&quot;dump scraped items into FILE (use - for stdout)&quot;)</span><br><span class="line">        parser.add_option(&quot;-t&quot;, &quot;--output-format&quot;, metavar=&quot;FORMAT&quot;,</span><br><span class="line">                          help=&quot;format to use for dumping items with -o&quot;)</span><br><span class="line"></span><br><span class="line">    def process_options(self, args, opts):</span><br><span class="line">        ScrapyCommand.process_options(self, args, opts)</span><br><span class="line">        try:</span><br><span class="line">            opts.spargs = arglist_to_dict(opts.spargs)</span><br><span class="line">        except ValueError:</span><br><span class="line">            raise UsageError(&quot;Invalid -a value, use -a NAME=VALUE&quot;, print_help=False)</span><br><span class="line">        if opts.output:</span><br><span class="line">            if opts.output == &apos;-&apos;:</span><br><span class="line">                self.settings.set(&apos;FEED_URI&apos;, &apos;stdout:&apos;, priority=&apos;cmdline&apos;)</span><br><span class="line">            else:</span><br><span class="line">                self.settings.set(&apos;FEED_URI&apos;, opts.output, priority=&apos;cmdline&apos;)</span><br><span class="line">            feed_exporters = without_none_values(</span><br><span class="line">                self.settings.getwithbase(&apos;FEED_EXPORTERS&apos;))</span><br><span class="line">            valid_output_formats = feed_exporters.keys()</span><br><span class="line">            if not opts.output_format:</span><br><span class="line">                opts.output_format = os.path.splitext(opts.output)[1].replace(&quot;.&quot;, &quot;&quot;)</span><br><span class="line">            if opts.output_format not in valid_output_formats:</span><br><span class="line">                raise UsageError(&quot;Unrecognized output format &apos;%s&apos;, set one&quot;</span><br><span class="line">                                 &quot; using the &apos;-t&apos; switch or as a file extension&quot;</span><br><span class="line">                                 &quot; from the supported list %s&quot; % (opts.output_format,</span><br><span class="line">                                                                  tuple(valid_output_formats)))</span><br><span class="line">            self.settings.set(&apos;FEED_FORMAT&apos;, opts.output_format, priority=&apos;cmdline&apos;)</span><br><span class="line"></span><br><span class="line">    def run(self, args, opts):</span><br><span class="line">        runner = self.crawler_process</span><br><span class="line">        for spidername in runner.spider_loader.list():</span><br><span class="line">            runner.crawl(spidername, **opts.spargs)</span><br><span class="line"></span><br><span class="line">        d = runner.join()</span><br><span class="line">        d.addBoth(lambda _: runner.stop())</span><br><span class="line">        runner.start()</span><br></pre></td></tr></table></figure></p>
<p>Using <code>scrapy crawlall</code> to start all spiders, utilizing more process to run spiders (one process per spider).</p>
<p>// TO BE CONTINUE</p>

      
    </div>
    <footer>
      
        
        
        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://chainkite.com/2016/11/05/python_spider_scrapy/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:chainkite.com">
  </form>
</div>

  

  
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 chainkite
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
