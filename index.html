<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>None.get</title>
  <meta name="author" content="chainkite">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="None.get"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="None.get" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">None.get</a></h1>
  <h2><a href="/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-11-05T14:22:00.000Z"><a href="/2016/11/05/python_spider_scrapy/">2016-11-05</a></time>
      
      
  
    <h1 class="title"><a href="/2016/11/05/python_spider_scrapy/"></a></h1>
  

    </header>
    <div class="entry">
      
        <p>Above all, I am a beginner in <code>python</code>. <code>scrapy</code> is a crawler framework in <code>python</code>, makes me write a crawler project quite easily in <code>python</code>. But, the program is not quite efficient at the begining. It’s built on <code>twisted</code>, which is async, but single-threaded…</p>
<p><strong>Start</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br><span class="line"></span><br><span class="line">scrapy startproject first_spider ~/works/first_spider</span><br></pre></td></tr></table></figure></p>
<p>Then you will get a project structure like this.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── first_spider</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── settings.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">└── scrapy.cfg</span><br></pre></td></tr></table></figure></p>
<p>Follow the structure, you will find it’s easy to implement your spider, because the only thing you need to consider is the logic you are going to crawl and save.</p>
<p><strong>There are a lot of tutorials teach you how to write a spider, so I will skip this and jump to something further</strong></p>
<p><strong>Speed up your spider</strong><br>What I found out when I run my spider with <code>scrapy crawl first_spider</code>, it is run in single-thread. Async is only working to prevent the parsing from downloading (prevent cpu stucking from io).<br>In order speed up my spider, I split my spider into several tiny spiders. (example below skip details)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── first_spider</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       ├── first_spiders.py</span><br><span class="line">│       └── support</span><br><span class="line">│           ├── __init__.py</span><br><span class="line">│           └── first_spider_support.py</span><br></pre></td></tr></table></figure></p>
<p>first_spider_support.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class FirstSpiderSupport:</span><br><span class="line">	def __init__(self):</span><br><span class="line">		pass</span><br><span class="line">	</span><br><span class="line">	def do_start_requests(self):</span><br><span class="line">		...</span><br><span class="line">	</span><br><span class="line">	def do_parse(self, response):</span><br><span class="line">		...</span><br></pre></td></tr></table></figure></p>
<p>first_spiders.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class FirstOneSpider(scrapy.Spider, FirstSpiderSupport):</span><br><span class="line">	name = &quot;first_one_spider&quot;</span><br><span class="line"></span><br><span class="line">	def start_requests(self):</span><br><span class="line">        return self.do_start_requests()</span><br><span class="line"></span><br><span class="line">	def parse(self, response):</span><br><span class="line">        return self.do_parse(response)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class FirstTwoSpider(scrapy.Spider, FirstSpiderSupport):</span><br><span class="line">	name = &quot;first_two_spider&quot;</span><br><span class="line"></span><br><span class="line">	def start_requests(self):</span><br><span class="line">        return self.do_start_requests()</span><br><span class="line"></span><br><span class="line">	def parse(self, response):</span><br><span class="line">        return self.do_parse(response)</span><br><span class="line">        </span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>Implement a <code>scrapy command</code> to run all spiders once.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── first_spider</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── commands</span><br><span class="line">│   │   ├── __init__.py</span><br><span class="line">│   │   └── crawlall.py</span><br></pre></td></tr></table></figure></p>
<p>crawlall.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from scrapy.commands import ScrapyCommand</span><br><span class="line">from scrapy.utils.conf import arglist_to_dict</span><br><span class="line">from scrapy.utils.python import without_none_values</span><br><span class="line">from scrapy.exceptions import UsageError</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line">class Command(ScrapyCommand):</span><br><span class="line"></span><br><span class="line">    requires_project = True</span><br><span class="line"></span><br><span class="line">    def syntax(self):</span><br><span class="line">        return &apos;[options]&apos;</span><br><span class="line"></span><br><span class="line">    def short_desc(self):</span><br><span class="line">        return &apos;Runs all of the spiders&apos;</span><br><span class="line"></span><br><span class="line">    def add_options(self, parser):</span><br><span class="line">        ScrapyCommand.add_options(self, parser)</span><br><span class="line">        parser.add_option(&quot;-a&quot;, dest=&quot;spargs&quot;, action=&quot;append&quot;, default=[], metavar=&quot;NAME=VALUE&quot;,</span><br><span class="line">                          help=&quot;set spider argument (may be repeated)&quot;)</span><br><span class="line">        parser.add_option(&quot;-o&quot;, &quot;--output&quot;, metavar=&quot;FILE&quot;,</span><br><span class="line">                          help=&quot;dump scraped items into FILE (use - for stdout)&quot;)</span><br><span class="line">        parser.add_option(&quot;-t&quot;, &quot;--output-format&quot;, metavar=&quot;FORMAT&quot;,</span><br><span class="line">                          help=&quot;format to use for dumping items with -o&quot;)</span><br><span class="line"></span><br><span class="line">    def process_options(self, args, opts):</span><br><span class="line">        ScrapyCommand.process_options(self, args, opts)</span><br><span class="line">        try:</span><br><span class="line">            opts.spargs = arglist_to_dict(opts.spargs)</span><br><span class="line">        except ValueError:</span><br><span class="line">            raise UsageError(&quot;Invalid -a value, use -a NAME=VALUE&quot;, print_help=False)</span><br><span class="line">        if opts.output:</span><br><span class="line">            if opts.output == &apos;-&apos;:</span><br><span class="line">                self.settings.set(&apos;FEED_URI&apos;, &apos;stdout:&apos;, priority=&apos;cmdline&apos;)</span><br><span class="line">            else:</span><br><span class="line">                self.settings.set(&apos;FEED_URI&apos;, opts.output, priority=&apos;cmdline&apos;)</span><br><span class="line">            feed_exporters = without_none_values(</span><br><span class="line">                self.settings.getwithbase(&apos;FEED_EXPORTERS&apos;))</span><br><span class="line">            valid_output_formats = feed_exporters.keys()</span><br><span class="line">            if not opts.output_format:</span><br><span class="line">                opts.output_format = os.path.splitext(opts.output)[1].replace(&quot;.&quot;, &quot;&quot;)</span><br><span class="line">            if opts.output_format not in valid_output_formats:</span><br><span class="line">                raise UsageError(&quot;Unrecognized output format &apos;%s&apos;, set one&quot;</span><br><span class="line">                                 &quot; using the &apos;-t&apos; switch or as a file extension&quot;</span><br><span class="line">                                 &quot; from the supported list %s&quot; % (opts.output_format,</span><br><span class="line">                                                                  tuple(valid_output_formats)))</span><br><span class="line">            self.settings.set(&apos;FEED_FORMAT&apos;, opts.output_format, priority=&apos;cmdline&apos;)</span><br><span class="line"></span><br><span class="line">    def run(self, args, opts):</span><br><span class="line">        runner = self.crawler_process</span><br><span class="line">        for spidername in runner.spider_loader.list():</span><br><span class="line">            runner.crawl(spidername, **opts.spargs)</span><br><span class="line"></span><br><span class="line">        d = runner.join()</span><br><span class="line">        d.addBoth(lambda _: runner.stop())</span><br><span class="line">        runner.start()</span><br></pre></td></tr></table></figure></p>
<p>Using <code>scrapy crawlall</code> to start all spiders, utilizing more process to run spiders (one process per spider).</p>
<p>// TO BE CONTINUE</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-06T14:22:00.000Z"><a href="/2016/07/06/first_shoot_in_python_world/">2016-07-06</a></time>
      
      
  
    <h1 class="title"><a href="/2016/07/06/first_shoot_in_python_world/"></a></h1>
  

    </header>
    <div class="entry">
      
        <p><strong>Version Manager</strong></p>
<p><code>pyenv</code>:</p>
<ul>
<li>port from <code>rbenv</code></li>
<li>it cannot find existed python versions in my mac</li>
<li>set application-specified version: <code>pyenv local x.x.x</code></li>
<li>set global version: <code>pyenv global x.x.x</code></li>
<li>detail instructions look at <a href="https://github.com/yyuu/pyenv/blob/master/COMMANDS.md" target="_blank" rel="external">command reference</a>, whose entry is so small and easy to be neglected in README.</li>
</ul>
<p><code>pip</code>: …hard to tell</p>
<ul>
<li>something like apt-get or homebrew, to install py dependencies…(in system level)</li>
</ul>
<p><strong>Build Tool</strong></p>
<p><code>setup.py</code> ??<br><code>easyinstall</code> + <code>virtualenv</code> ?? no idea</p>
<p><strong>Noteworthy</strong></p>
<ul>
<li>When a module named <code>spam</code> is imported, the interpreter first searches for a built-in module with that name. If not found, it then searches for a file named <code>spam.py</code> in a list of directories given by the variable <code>sys.path</code>.</li>
<li><code>sys.path</code> is initialized from these locations:<ul>
<li>The directory containing the input script (or the current directory when no file is specified).</li>
<li><code>PYTHONPATH</code> (a list of directory names, with the same syntax as the shell variable <code>PATH</code>).</li>
<li>The installation-dependent default.</li>
</ul>
</li>
<li>Python caches the compiled version of each module in the <code>__pycache__</code> directory under the name <code>module.version.pyc</code></li>
<li><code>from foo import bar</code>. With <code>__all__</code> in <code>__init__.py</code>, <code>import *</code> will import those named submodules in <code>__all__</code>. Otherwise, it only ensures that the package <code>foo</code> has been imported.<br>  <code>__init__.py</code> will be loaded when import.</li>
</ul>
<ul>
<li><code>print(&#39;xxx&#39;, end = &#39;x&#39;)</code> usually <code>print</code> is ended by <code>&#39;\n&#39;</code>, you can specify a charactor to replace it.</li>
<li><code>open(file, mode=&#39;r&#39;, buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)</code><ul>
<li><code>&#39;r&#39;</code> open only for read</li>
<li><code>&#39;w&#39;</code> open only for write</li>
<li><code>&#39;x&#39;</code> open for exclusive creation, failing if the file already exists</li>
<li><code>&#39;a&#39;</code> open for write (only appending)</li>
<li><code>&#39;b&#39;</code> open a file in binary mode</li>
<li><code>&#39;t&#39;</code> text mode</li>
<li><code>&#39;+&#39;</code> read and write</li>
</ul>
</li>
<li><code>f.seek(f.tell)</code></li>
<li><code>json.dumps(obj)</code> &lt;-&gt; <code>json.load(str)</code></li>
<li><code>pickle</code>: serialize and deserialize python objects (like java.io.Serializable)</li>
<li></li>
</ul>
<hr>
<p><strong>REF</strong><br><a href="https://github.com/yyuu/pyenv" target="_blank" rel="external">pyenv</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-27T15:18:00.000Z"><a href="/2016/06/27/first_shoot_in_js_world/">2016-06-27</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/27/first_shoot_in_js_world/">First Shoot in Javascript World</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Trying hard to get into the js world…<br>Then, being killed</p>
<p><strong>Version Manager</strong></p>
<p><code>nvm</code>:</p>
<ul>
<li>a version manager for <code>Node.js</code>, easily switch current version of node</li>
<li>In js world (type unsafe), things seem not very compatible with historical versions.</li>
</ul>
<p><strong>Build Tool</strong></p>
<p><code>npm</code>:</p>
<ul>
<li>mostly for back-end stuffs (Node.js)</li>
<li>nested dependencies management</li>
</ul>
<p><code>bower</code>: </p>
<ul>
<li>strictly, not a build tool, but a package manager</li>
<li>mostly for front-end stuffs (CSS/HTML Templates/…)</li>
<li>flat dependencies management (it’s not acceptable there are 3 jQuery in your front-end project)</li>
<li>BTW, we install <code>bower</code> through <code>npm</code>, hahaha</li>
</ul>
<p><code>gulp</code>:</p>
<ul>
<li>a task runner, install through <code>npm</code> as well</li>
<li>plenty of plugins</li>
<li>usage example: minify all the js/css files and so on</li>
</ul>
<p><em>p.s.</em><br>In <code>scala</code> world, <code>sbt</code> and its plugins takeover everything above.</p>
<hr>
<p><strong>Test</strong></p>
<p><code>jasmine</code>: Behavior-Driven Development Framework</p>
<p><code>karma</code>:</p>
<ul>
<li>test runner</li>
<li>use the browser js engine to test</li>
</ul>
<p><code>protractor</code>:</p>
<ul>
<li>angular’s e2e test framework, with selenium</li>
<li>real and live browser test</li>
</ul>
<hr>
<p><strong>AngularJS</strong></p>
<p>Module -&gt; Directive -&gt; Component</p>
<p><strong>ReactJS</strong></p>
<p><strong><em>To Be Continue</em></strong></p>
<hr>
<p><strong>REF</strong><br><a href="https://github.com/creationix/nvm" target="_blank" rel="external">nvm</a><br><a href="http://bower.io" target="_blank" rel="external">bower</a><br><a href="https://www.npmjs.com" target="_blank" rel="external">npm</a><br><a href="http://ng-learn.org/2013/11/Bower-vs-npm/" target="_blank" rel="external">bower vs. npm</a><br><a href="https://github.com/jasmine/jasmine" target="_blank" rel="external">jasmine</a><br><a href="https://github.com/karma-runner/karma" target="_blank" rel="external">karma</a><br><a href="http://andy-carter.com/blog/a-beginners-guide-to-the-task-runner-gulp" target="_blank" rel="external">A Beginners Guide to the Task Runner Gulp</a><br><a href="http://andy-carter.com/blog/a-beginners-guide-to-package-manager-bower-and-using-gulp-to-manage-components" target="_blank" rel="external">A Beginners Guide to Package Manager Bower and Using Gulp to Manage Components</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-09T09:49:00.000Z"><a href="/2016/06/09/why_play_java/">2016-06-09</a></time>
      
      
  
    <h1 class="title"><a href="/2016/06/09/why_play_java/">Why I use Play Framework even in Java</a></h1>
  

    </header>
    <div class="entry">
      
        <ul>
<li><p><strong>Type safe</strong> even in template</p>
<ul>
<li>Catch the bug in compile.</li>
<li>Reduce code rot, more readable</li>
</ul>
</li>
<li><p><strong>Theaded vs. Evented</strong></p>
<ul>
<li><strong>SpringMVC</strong> is Threaded: one thread per request, blocking</li>
<li><p><strong>Play</strong> is Evented: async, non-blocking io</p>
<p>In the SOA / Microservice world, a lot of requests between services, the blocking SpringMVC is not a goot choice.</p>
</li>
</ul>
</li>
<li><p><strong>play.libs.F</strong></p>
<ul>
<li>Package with functional methods (compromise for scala guys)</li>
<li>F.Promise is async, java.current.Future may block</li>
</ul>
</li>
<li><p><strong>Hot reload</strong></p>
</li>
<li><p><strong>sbt dist</strong> assemble a executable jar with java env</p>
<ul>
<li>No need a container like tomcat, jboss</li>
<li>Easy deploy, scale and failover</li>
</ul>
</li>
</ul>
<p>-<br><a href="https://www.youtube.com/watch?v=8z3h4Uv9YbE" target="_blank" rel="external">Play in Linkedin</a> (although it was shared two years ago, play is even better now)</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-12T09:52:00.000Z"><a href="/2016/05/12/scala_hk_meet_up_#1/">2016-05-12</a></time>
      
      
  
    <h1 class="title"><a href="/2016/05/12/scala_hk_meet_up_#1/">Scala Meet-up in HK</a></h1>
  

    </header>
    <div class="entry">
      
        <p>The meet-up mainly introduces scala to non-scala programmers.<br>No slides shared yet.<br>Hold by oneskyapp.com on 11 May 2016.</p>
<p><strong>First Part:</strong><br>Introduce some attractive scala features, types, syntax (like functional, immutable, Option, for / yield …)</p>
<p><strong>Second Part:</strong><br>Introduce <code>Future</code> and <code>Akka-Stream</code>.</p>
<p><code>Future</code>: only talk about usage, nothing to note.</p>
<p><code>Akka-Stream</code>: easily manage workflows (throttling, up-down streams, …).</p>
<p><strong>More after the meet-up:</strong><br>Onesky use <strong>RabbitMQ</strong> for most inter-service communications, <strong>Kubernetes</strong> for docker cluster management.<br>More about their <a href="https://medium.com/translate-engineer-error/our-stack-from-monolithic-php-to-microservices-scala-a7950266c4be" target="_blank" rel="external">tech-stack</a><br>Netflix is going to open-source <strong>falcor-java.</strong> </p>
<p>other keywords: gRPC, delis</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:chainkite.com">
  </form>
</div>

  

  
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 chainkite
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
